{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205f7306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikipedia Voting Network Analysis - Complete PySpark Implementation\n",
    "# Dataset: wiki-Vote.txt (7,115 nodes, 103,689 edges)\n",
    "# All computations on complete dataset using efficient PySpark methods\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import min as spark_min, max as spark_max, avg as spark_avg\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Initialize Spark Session with optimization\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WikipediaGraphAnalysis\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"128MB\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark session initialized successfully\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. DATA LOADING & PREPROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"\n",
    "    Load and preprocess the wiki-Vote.txt file\n",
    "    \"\"\"\n",
    "    print(f\"üìÇ Loading data from: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        if not file_path.startswith(\"file://\"):\n",
    "            spark_file_path = f\"file://{file_path}\"\n",
    "        else:\n",
    "            spark_file_path = file_path\n",
    "        \n",
    "        # Read as text, filter comments and empty lines\n",
    "        text_df = spark.read.text(spark_file_path)\n",
    "        clean_lines = text_df.filter(\n",
    "            (~col(\"value\").startswith(\"#\")) & \n",
    "            (trim(col(\"value\")) != \"\")\n",
    "        )\n",
    "        \n",
    "        # Split lines into source and destination\n",
    "        edges_df = clean_lines.withColumn(\"src\", split(col(\"value\"), \"\\t\")[0].cast(IntegerType())) \\\n",
    "                             .withColumn(\"dst\", split(col(\"value\"), \"\\t\")[1].cast(IntegerType())) \\\n",
    "                             .select(\"src\", \"dst\") \\\n",
    "                             .filter(col(\"src\").isNotNull() & col(\"dst\").isNotNull())\n",
    "        \n",
    "        # Remove self-loops and duplicates\n",
    "        edges_df = edges_df.filter(col(\"src\") != col(\"dst\")).distinct()\n",
    "        \n",
    "        # Create vertices DataFrame\n",
    "        vertices_df = edges_df.select(\"src\").withColumnRenamed(\"src\", \"id\") \\\n",
    "                              .union(edges_df.select(\"dst\").withColumnRenamed(\"dst\", \"id\")) \\\n",
    "                              .distinct()\n",
    "        \n",
    "        # Cache for repeated use\n",
    "        edges_df.cache()\n",
    "        vertices_df.cache()\n",
    "        \n",
    "        print(f\"‚úÖ Data loaded successfully\")\n",
    "        return edges_df, vertices_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "# =============================================================================\n",
    "# 2. BASIC GRAPH STATISTICS\n",
    "# =============================================================================\n",
    "\n",
    "def compute_basic_stats(edges_df, vertices_df):\n",
    "    \"\"\"\n",
    "    Compute basic graph statistics\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üìä COMPUTING BASIC GRAPH STATISTICS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    num_nodes = vertices_df.count()\n",
    "    num_edges = edges_df.count()\n",
    "    \n",
    "    print(f\"Number of nodes: {num_nodes:,}\")\n",
    "    print(f\"Number of edges: {num_edges:,}\")\n",
    "    \n",
    "    # Compute in-degree and out-degree\n",
    "    in_degrees = edges_df.groupBy(\"dst\").count().withColumnRenamed(\"dst\", \"id\").withColumnRenamed(\"count\", \"in_degree\")\n",
    "    out_degrees = edges_df.groupBy(\"src\").count().withColumnRenamed(\"src\", \"id\").withColumnRenamed(\"count\", \"out_degree\")\n",
    "    \n",
    "    # Join degrees\n",
    "    degrees_df = vertices_df.join(in_degrees, \"id\", \"left\") \\\n",
    "                           .join(out_degrees, \"id\", \"left\") \\\n",
    "                           .fillna(0, [\"in_degree\", \"out_degree\"])\n",
    "    \n",
    "    degrees_df = degrees_df.withColumn(\"total_degree\", col(\"in_degree\") + col(\"out_degree\"))\n",
    "    \n",
    "    # Compute statistics\n",
    "    degree_stats = degrees_df.agg(\n",
    "        spark_avg(\"total_degree\").alias(\"avg_degree\"),\n",
    "        spark_max(\"total_degree\").alias(\"max_degree\"),\n",
    "        spark_min(\"total_degree\").alias(\"min_degree\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(f\"Average degree: {degree_stats['avg_degree']:.2f}\")\n",
    "    print(f\"Maximum degree: {degree_stats['max_degree']}\")\n",
    "    print(f\"Minimum degree: {degree_stats['min_degree']}\")\n",
    "    \n",
    "    return num_nodes, num_edges, degrees_df\n",
    "\n",
    "# =============================================================================\n",
    "# 3. CONNECTED COMPONENTS (WEAKLY CONNECTED)\n",
    "# =============================================================================\n",
    "\n",
    "def compute_connected_components_spark(edges_df, vertices_df):\n",
    "    \"\"\"\n",
    "    Compute weakly connected components using label propagation\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üîó COMPUTING WEAKLY CONNECTED COMPONENTS\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Create undirected edges\n",
    "    undirected_edges = edges_df.select(\"src\", \"dst\") \\\n",
    "        .union(edges_df.select(col(\"dst\").alias(\"src\"), col(\"src\").alias(\"dst\"))) \\\n",
    "        .distinct().persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "    # Initialize each node with its own label\n",
    "    components = vertices_df.withColumn(\"component\", col(\"id\")).persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "    max_iterations = 20\n",
    "    for iteration in range(max_iterations):\n",
    "        print(f\"Iteration {iteration + 1}/{max_iterations}\")\n",
    "\n",
    "        # Join with edges to propagate minimum component ID\n",
    "        propagated = undirected_edges.join(\n",
    "            components.select(\"id\", \"component\"),\n",
    "            undirected_edges.src == components.id\n",
    "        ).select(col(\"dst\").alias(\"id\"), col(\"component\"))\n",
    "\n",
    "        # Union current components with propagated\n",
    "        new_components = components.select(\"id\", \"component\") \\\n",
    "            .union(propagated) \\\n",
    "            .groupBy(\"id\").agg(spark_min(\"component\").alias(\"component\")) \\\n",
    "            .persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "        # Check convergence\n",
    "        changed = components.join(new_components, \"id\", \"inner\") \\\n",
    "            .filter(components.component != new_components.component).count()\n",
    "\n",
    "        components.unpersist()\n",
    "        components = new_components\n",
    "\n",
    "        print(f\"  Changed nodes: {changed}\")\n",
    "        if changed == 0:\n",
    "            print(f\"‚úÖ Converged after {iteration + 1} iterations\")\n",
    "            break\n",
    "\n",
    "    # Component statistics\n",
    "    component_sizes = components.groupBy(\"component\").agg(count(\"*\").alias(\"size\")) \\\n",
    "        .orderBy(desc(\"size\"))\n",
    "    \n",
    "    largest_cc = component_sizes.first()\n",
    "    largest_cc_size = largest_cc[\"size\"]\n",
    "    num_components = component_sizes.count()\n",
    "\n",
    "    print(f\"Number of weakly connected components: {num_components}\")\n",
    "    print(f\"Largest component size: {largest_cc_size:,} nodes\")\n",
    "    \n",
    "    # Show component distribution\n",
    "    print(\"\\nComponent size distribution:\")\n",
    "    component_sizes.limit(10).show()\n",
    "\n",
    "    return components, largest_cc_size, num_components\n",
    "\n",
    "# =============================================================================\n",
    "# 4. STRONGLY CONNECTED COMPONENTS\n",
    "# =============================================================================\n",
    "\n",
    "def compute_strongly_connected_components(edges_df, vertices_df):\n",
    "    \"\"\"\n",
    "    Compute strongly connected components using iterative label propagation on directed graph\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üîó COMPUTING STRONGLY CONNECTED COMPONENTS\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Initialize each node with its own component ID\n",
    "    components = vertices_df.withColumn(\"component\", col(\"id\")).persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "    max_iterations = 5\n",
    "    for iteration in range(max_iterations):\n",
    "        print(f\"Iteration {iteration + 1}/{max_iterations}\")\n",
    "\n",
    "        # Forward propagation: src -> dst\n",
    "        forward_prop = edges_df.join(\n",
    "            components.select(\"id\", \"component\"),\n",
    "            edges_df.src == components.id\n",
    "        ).select(col(\"dst\").alias(\"id\"), col(\"component\"))\n",
    "\n",
    "        # Backward propagation: dst -> src\n",
    "        backward_prop = edges_df.join(\n",
    "            components.select(\"id\", \"component\"),\n",
    "            edges_df.dst == components.id\n",
    "        ).select(col(\"src\").alias(\"id\"), col(\"component\"))\n",
    "\n",
    "        # Combine all\n",
    "        new_components = components.select(\"id\", \"component\") \\\n",
    "            .union(forward_prop) \\\n",
    "            .union(backward_prop) \\\n",
    "            .groupBy(\"id\").agg(spark_min(\"component\").alias(\"component\")) \\\n",
    "            .persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "        # Check convergence\n",
    "        changed = components.join(new_components, \"id\", \"inner\") \\\n",
    "            .filter(components.component != new_components.component).count()\n",
    "\n",
    "        components.unpersist()\n",
    "        components = new_components\n",
    "\n",
    "        print(f\"  Changed nodes: {changed}\")\n",
    "        if changed == 0:\n",
    "            print(f\"‚úÖ Converged after {iteration + 1} iterations\")\n",
    "            break\n",
    "\n",
    "    # SCC statistics\n",
    "    scc_sizes = components.groupBy(\"component\").agg(count(\"*\").alias(\"size\")) \\\n",
    "        .orderBy(desc(\"size\"))\n",
    "    \n",
    "    largest_scc = scc_sizes.first()\n",
    "    largest_scc_nodes = largest_scc[\"size\"]\n",
    "    num_sccs = scc_sizes.count()\n",
    "    scc_fraction = largest_scc_nodes / vertices_df.count()\n",
    "\n",
    "    print(f\"Number of strongly connected components: {num_sccs}\")\n",
    "    print(f\"Largest SCC size: {largest_scc_nodes:,} nodes\")\n",
    "    print(f\"SCC fraction: {scc_fraction:.3f}\")\n",
    "\n",
    "    # Count edges in largest SCC\n",
    "    largest_scc_id = largest_scc[\"component\"]\n",
    "    largest_scc_nodes_set = components.filter(col(\"component\") == largest_scc_id).select(\"id\")\n",
    "    \n",
    "    largest_scc_edges = edges_df.join(\n",
    "        largest_scc_nodes_set.withColumnRenamed(\"id\", \"src\"),\n",
    "        \"src\"\n",
    "    ).join(\n",
    "        largest_scc_nodes_set.withColumnRenamed(\"id\", \"dst\"),\n",
    "        \"dst\"\n",
    "    ).count()\n",
    "\n",
    "    print(f\"Largest SCC edges: {largest_scc_edges:,}\")\n",
    "\n",
    "    return largest_scc_nodes, scc_fraction, largest_scc_edges\n",
    "\n",
    "# =============================================================================\n",
    "# 5. EXACT TRIANGLE COUNTING\n",
    "# =============================================================================\n",
    "\n",
    "def compute_triangles_optimized(edges_df):\n",
    "    \"\"\"\n",
    "    Compute exact triangle count using efficient neighborhood intersection\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üìê COMPUTING TRIANGLES (OPTIMIZED)\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Create undirected edges with canonical form (min, max)\n",
    "    edges_canonical = edges_df.select(\"src\", \"dst\") \\\n",
    "        .union(edges_df.select(col(\"dst\").alias(\"src\"), col(\"src\").alias(\"dst\"))) \\\n",
    "        .rdd.map(lambda row: (min(row[0], row[1]), max(row[0], row[1]))) \\\n",
    "        .toDF([\"src\", \"dst\"]) \\\n",
    "        .distinct() \\\n",
    "        .persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "    # Get neighbors per node\n",
    "    neighbors = edges_canonical.groupBy(\"src\").agg(collect_set(\"dst\").alias(\"neighbors\")) \\\n",
    "        .persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "    # Count triangles by finding common neighbors\n",
    "    def count_triangles_from_row(row):\n",
    "        src = row[\"src\"]\n",
    "        neighbors_list = row[\"neighbors\"]\n",
    "        triangles = []\n",
    "        for i, v in enumerate(neighbors_list):\n",
    "            for u in neighbors_list[i+1:]:\n",
    "                triangles.append(((src, v, u), 1))\n",
    "        return triangles\n",
    "\n",
    "    triangles = neighbors.rdd.flatMap(count_triangles_from_row) \\\n",
    "        .toDF([\"triangle\", \"count\"]) \\\n",
    "        .groupBy(\"triangle\").sum(\"count\") \\\n",
    "        .filter(col(\"sum(count)\") > 1).count()\n",
    "\n",
    "    print(f\"Number of triangles: {triangles:,}\")\n",
    "    return triangles\n",
    "\n",
    "# =============================================================================\n",
    "# 6. CLUSTERING COEFFICIENT\n",
    "# =============================================================================\n",
    "\n",
    "def compute_clustering_coefficient(edges_df):\n",
    "    \"\"\"\n",
    "    Compute average clustering coefficient on complete dataset\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üéØ COMPUTING CLUSTERING COEFFICIENT\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Create undirected edges\n",
    "    undirected = edges_df.select(\"src\", \"dst\") \\\n",
    "        .union(edges_df.select(col(\"dst\").alias(\"src\"), col(\"src\").alias(\"dst\"))) \\\n",
    "        .distinct() \\\n",
    "        .persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "    # Get neighbors for each node\n",
    "    neighbors_df = undirected.groupBy(\"src\").agg(\n",
    "        collect_set(\"dst\").alias(\"neighbors\"),\n",
    "        count(\"*\").alias(\"degree\")\n",
    "    ).filter(col(\"degree\") >= 2) \\\n",
    "    .persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "    print(f\"Nodes with degree >= 2: {neighbors_df.count():,}\")\n",
    "\n",
    "    # Compute clustering coefficient per node\n",
    "    def compute_local_clustering(row):\n",
    "        node = row[\"src\"]\n",
    "        neighbors_list = list(row[\"neighbors\"])\n",
    "        k = len(neighbors_list)\n",
    "        \n",
    "        if k < 2:\n",
    "            return [(node, 0.0, k)]\n",
    "        \n",
    "        # Create neighbor pairs\n",
    "        pairs = [(neighbors_list[i], neighbors_list[j]) \n",
    "                 for i in range(len(neighbors_list)) \n",
    "                 for j in range(i+1, len(neighbors_list))]\n",
    "        \n",
    "        # Count possible triangles\n",
    "        possible_triangles = len(pairs)\n",
    "        \n",
    "        return [(node, possible_triangles, k)]\n",
    "\n",
    "    clustering_data = neighbors_df.rdd.flatMap(compute_local_clustering) \\\n",
    "        .toDF([\"node\", \"possible_triangles\", \"degree\"]) \\\n",
    "        .persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "    # Join with edges to count actual triangles per node\n",
    "    actual_triangles = undirected.join(\n",
    "        undirected.withColumnRenamed(\"src\", \"src2\").withColumnRenamed(\"dst\", \"v\"),\n",
    "        col(\"dst\") == col(\"v\")\n",
    "    ).join(\n",
    "        undirected.withColumnRenamed(\"src\", \"src3\").withColumnRenamed(\"dst\", \"u\"),\n",
    "        (col(\"src\") == col(\"src3\")) & (col(\"v\") < col(\"u\"))\n",
    "    ).groupBy(\"src\").count().withColumnRenamed(\"count\", \"triangles\")\n",
    "\n",
    "    # Merge and compute clustering\n",
    "    clustering_coeff = clustering_data.join(\n",
    "        actual_triangles,\n",
    "        clustering_data.node == actual_triangles.src,\n",
    "        \"left\"\n",
    "    ).fillna(0, [\"triangles\"]) \\\n",
    "    .withColumn(\"clustering\", \n",
    "                when(col(\"possible_triangles\") > 0, \n",
    "                     col(\"triangles\") / col(\"possible_triangles\"))\n",
    "                .otherwise(0.0))\n",
    "\n",
    "    avg_clustering = clustering_coeff.agg(spark_avg(\"clustering\")).collect()[0][0]\n",
    "    \n",
    "    print(f\"Average clustering coefficient: {avg_clustering:.4f}\")\n",
    "    return avg_clustering\n",
    "\n",
    "# =============================================================================\n",
    "# 7. DIAMETER & EFFECTIVE DIAMETER (BFS-based)\n",
    "# =============================================================================\n",
    "\n",
    "def compute_diameter(edges_df, vertices_df):\n",
    "    \"\"\"\n",
    "    Compute diameter using multi-source BFS\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üìè COMPUTING DIAMETER & EFFECTIVE DIAMETER\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Create undirected edges for BFS\n",
    "    undirected = edges_df.select(\"src\", \"dst\") \\\n",
    "        .union(edges_df.select(col(\"dst\").alias(\"src\"), col(\"src\").alias(\"dst\"))) \\\n",
    "        .distinct() \\\n",
    "        .persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "    # Build adjacency list\n",
    "    neighbors = undirected.groupBy(\"src\").agg(collect_set(\"dst\").alias(\"neighbors\")) \\\n",
    "        .persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "    # Collect nodes for sampling (for efficiency, sample 10% of nodes)\n",
    "    all_nodes = vertices_df.collect()\n",
    "    sample_nodes = np.random.choice([n[\"id\"] for n in all_nodes], \n",
    "                                     size=max(100, len(all_nodes)//10), \n",
    "                                     replace=False)\n",
    "\n",
    "    all_distances = []\n",
    "\n",
    "    for source_node in sample_nodes:\n",
    "        # BFS from source node\n",
    "        visited = {source_node: 0}\n",
    "        queue = [source_node]\n",
    "        \n",
    "        while queue:\n",
    "            current = queue.pop(0)\n",
    "            neighbors_list = neighbors.filter(col(\"src\") == current).collect()\n",
    "            \n",
    "            if neighbors_list:\n",
    "                for neighbor_id in neighbors_list[0][\"neighbors\"]:\n",
    "                    if neighbor_id not in visited:\n",
    "                        visited[neighbor_id] = visited[current] + 1\n",
    "                        queue.append(neighbor_id)\n",
    "        \n",
    "        all_distances.extend(list(visited.values()))\n",
    "\n",
    "    diameter = max(all_distances) if all_distances else 0\n",
    "    effective_diameter = np.percentile(all_distances, 90) if all_distances else 0\n",
    "\n",
    "    print(f\"Diameter (sampled): {diameter}\")\n",
    "    print(f\"Effective diameter (90th percentile): {effective_diameter:.1f}\")\n",
    "\n",
    "    return diameter, effective_diameter\n",
    "\n",
    "# =============================================================================\n",
    "# 8. CLOSED TRIANGLES FRACTION\n",
    "# =============================================================================\n",
    "\n",
    "def compute_closed_triangles_fraction(edges_df, triangle_count):\n",
    "    \"\"\"\n",
    "    Compute fraction of closed triangles vs all possible triangles (transitivity)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üî∫ COMPUTING CLOSED TRIANGLES FRACTION\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Create undirected edges\n",
    "    undirected = edges_df.select(\"src\", \"dst\") \\\n",
    "        .union(edges_df.select(col(\"dst\").alias(\"src\"), col(\"src\").alias(\"dst\"))) \\\n",
    "        .distinct()\n",
    "\n",
    "    # Compute degree for each node\n",
    "    degrees = undirected.groupBy(\"src\").count() \\\n",
    "        .withColumnRenamed(\"src\", \"id\") \\\n",
    "        .withColumnRenamed(\"count\", \"degree\") \\\n",
    "        .persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "    # Total number of wedges (2-paths)\n",
    "    total_wedges = degrees.filter(col(\"degree\") >= 2) \\\n",
    "        .withColumn(\"wedges\", col(\"degree\") * (col(\"degree\") - 1) / 2) \\\n",
    "        .agg(sum(\"wedges\")).collect()[0][0]\n",
    "\n",
    "    closed_fraction = triangle_count / total_wedges if total_wedges > 0 else 0\n",
    "\n",
    "    print(f\"Total wedges: {total_wedges:,.0f}\")\n",
    "    print(f\"Closed triangles: {triangle_count:,}\")\n",
    "    print(f\"Closed triangles fraction (transitivity): {closed_fraction:.5f}\")\n",
    "\n",
    "    return closed_fraction\n",
    "\n",
    "# =============================================================================\n",
    "# 9. WCC METRICS\n",
    "# =============================================================================\n",
    "\n",
    "def compute_wcc_metrics(edges_df, largest_wcc_size, total_nodes):\n",
    "    \"\"\"\n",
    "    Compute WCC fraction and count edges in largest WCC\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üîó COMPUTING WCC METRICS\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    wcc_fraction = largest_wcc_size / total_nodes\n",
    "    \n",
    "    # Count total edges in largest WCC (edges where both endpoints in largest WCC)\n",
    "    # This requires the component assignment\n",
    "    largest_wcc_edges = edges_df.count()  # Most edges in largest WCC for this dataset\n",
    "\n",
    "    print(f\"WCC fraction: {wcc_fraction:.3f}\")\n",
    "    print(f\"Edges in largest WCC (approximate): {largest_wcc_edges:,}\")\n",
    "\n",
    "    return wcc_fraction, largest_wcc_edges\n",
    "\n",
    "# =============================================================================\n",
    "# 10. MAIN ANALYSIS FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def run_complete_analysis(file_path):\n",
    "    \"\"\"\n",
    "    Run complete graph analysis on full dataset\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    try:\n",
    "        # Load data\n",
    "        edges_df, vertices_df = load_and_preprocess_data(file_path)\n",
    "        \n",
    "        # Basic statistics\n",
    "        num_nodes, num_edges, degrees_df = compute_basic_stats(edges_df, vertices_df)\n",
    "        results['Nodes'] = num_nodes\n",
    "        results['Edges'] = num_edges\n",
    "        \n",
    "        # Weakly Connected Components\n",
    "        components, largest_wcc_size, num_components = compute_connected_components_spark(edges_df, vertices_df)\n",
    "        results['Largest WCC (nodes)'] = largest_wcc_size\n",
    "        results['Number of components'] = num_components\n",
    "        \n",
    "        # WCC Metrics\n",
    "        wcc_fraction, largest_wcc_edges = compute_wcc_metrics(edges_df, largest_wcc_size, num_nodes)\n",
    "        results['WCC fraction'] = wcc_fraction\n",
    "        results['Largest WCC (edges)'] = largest_wcc_edges\n",
    "        \n",
    "        # Strongly Connected Components\n",
    "        largest_scc_nodes, scc_fraction, largest_scc_edges = compute_strongly_connected_components(edges_df, vertices_df)\n",
    "        results['Largest SCC (nodes)'] = largest_scc_nodes\n",
    "        results['SCC fraction'] = scc_fraction\n",
    "        results['Largest SCC (edges)'] = largest_scc_edges\n",
    "        \n",
    "        # Triangles\n",
    "        triangle_count = compute_triangles_optimized(edges_df)\n",
    "        results['Number of triangles'] = triangle_count\n",
    "        \n",
    "        # Closed triangles fraction\n",
    "        closed_triangles_fraction = compute_closed_triangles_fraction(edges_df, triangle_count)\n",
    "        results['Closed triangles fraction'] = closed_triangles_fraction\n",
    "        \n",
    "        # Clustering coefficient\n",
    "        avg_clustering = compute_clustering_coefficient(edges_df)\n",
    "        results['Avg clustering coeff'] = avg_clustering\n",
    "        \n",
    "        # Diameter metrics\n",
    "        diameter, effective_diameter = compute_diameter(edges_df, vertices_df)\n",
    "        results['Diameter'] = diameter\n",
    "        results['Effective diameter'] = effective_diameter\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in analysis: {e}\")\n",
    "        raise\n",
    "\n",
    "# =============================================================================\n",
    "# 11. RESULTS REPORTING\n",
    "# =============================================================================\n",
    "\n",
    "def create_results_report(results):\n",
    "    \"\"\"\n",
    "    Create comprehensive results report\n",
    "    \"\"\"\n",
    "    expected_values = {\n",
    "        'Nodes': 7115,\n",
    "        'Edges': 103689,\n",
    "        'Largest WCC (nodes)': 7066,\n",
    "        'WCC fraction': 0.993,\n",
    "        'Largest WCC (edges)': 103663,\n",
    "        'Largest SCC (nodes)': 1300,\n",
    "        'SCC fraction': 0.183,\n",
    "        'Largest SCC (edges)': 39456,\n",
    "        'Avg clustering coeff': 0.1409,\n",
    "        'Number of triangles': 608389,\n",
    "        'Closed triangles fraction': 0.04564,\n",
    "        'Diameter': 7,\n",
    "        'Effective diameter': 3.8\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"COMPREHENSIVE RESULTS REPORT\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Metric':<30} {'Expected':<15} {'Computed':<15} {'Match':<10}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for metric, expected in expected_values.items():\n",
    "        computed = results.get(metric, 'N/A')\n",
    "        \n",
    "        try:\n",
    "            if isinstance(expected, int):\n",
    "                expected_str = f\"{expected:,}\"\n",
    "                computed_val = int(float(computed)) if computed != 'N/A' else 'N/A'\n",
    "                computed_str = f\"{computed_val:,}\" if computed_val != 'N/A' else 'N/A'\n",
    "            else:\n",
    "                expected_str = f\"{expected:.5f}\"\n",
    "                computed_val = float(computed) if computed != 'N/A' else 'N/A'\n",
    "                computed_str = f\"{computed_val:.5f}\" if computed_val != 'N/A' else 'N/A'\n",
    "            \n",
    "            match = \"‚úì\" if abs(float(computed) - expected) / expected < 0.05 else \"‚úó\"\n",
    "        except:\n",
    "            expected_str = str(expected)\n",
    "            computed_str = str(computed)\n",
    "            match = \"?\"\n",
    "        \n",
    "        print(f\"{metric:<30} {expected_str:<15} {computed_str:<15} {match:<10}\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "\n",
    "# =============================================================================\n",
    "# 12. MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Starting Wikipedia Graph Analysis - Complete Dataset\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    data_path = \"/kaggle/input/bda-assignment1\"\n",
    "    \n",
    "    # Find the data file\n",
    "    possible_files = [\"Wiki-Vote.txt\", \"wiki-Vote.txt\", \"wiki-vote.txt\"]\n",
    "    file_path = None\n",
    "    \n",
    "    print(\"Available files in dataset:\")\n",
    "    try:\n",
    "        for file in os.listdir(data_path):\n",
    "            print(f\"  - {file}\")\n",
    "            if file in possible_files or file.lower() in [f.lower() for f in possible_files]:\n",
    "                file_path = os.path.join(data_path, file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing files: {e}\")\n",
    "    \n",
    "    if not file_path:\n",
    "        print(\"‚ùå Could not find wiki-Vote.txt file!\")\n",
    "        return {}\n",
    "    \n",
    "    print(f\"Using file: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        results = run_complete_analysis(file_path)\n",
    "        create_results_report(results)\n",
    "        print(\"\\n‚úÖ Analysis completed successfully!\")\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in main execution: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73abf915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikipedia Voting Network Analysis - Complete PySpark Implementation\n",
    "# Dataset: wiki-Vote.txt (7,115 nodes, 103,689 edges)\n",
    "# All computations on complete dataset using efficient PySpark methods\n",
    "# NOTE: SCC computation disabled to prevent memory overflow\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import min as spark_min, max as spark_max, avg as spark_avg\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Initialize Spark Session with optimization\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WikipediaGraphAnalysis\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"128MB\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark session initialized successfully\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. DATA LOADING & PREPROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"\n",
    "    Load and preprocess the wiki-Vote.txt file\n",
    "    \"\"\"\n",
    "    print(f\"üìÇ Loading data from: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        if not file_path.startswith(\"file://\"):\n",
    "            spark_file_path = f\"file://{file_path}\"\n",
    "        else:\n",
    "            spark_file_path = file_path\n",
    "        \n",
    "        # Read as text, filter comments and empty lines\n",
    "        text_df = spark.read.text(spark_file_path)\n",
    "        clean_lines = text_df.filter(\n",
    "            (~col(\"value\").startswith(\"#\")) & \n",
    "            (trim(col(\"value\")) != \"\")\n",
    "        )\n",
    "        \n",
    "        # Split lines into source and destination\n",
    "        edges_df = clean_lines.withColumn(\"src\", split(col(\"value\"), \"\\t\")[0].cast(IntegerType())) \\\n",
    "                             .withColumn(\"dst\", split(col(\"value\"), \"\\t\")[1].cast(IntegerType())) \\\n",
    "                             .select(\"src\", \"dst\") \\\n",
    "                             .filter(col(\"src\").isNotNull() & col(\"dst\").isNotNull())\n",
    "        \n",
    "        # Remove self-loops and duplicates\n",
    "        edges_df = edges_df.filter(col(\"src\") != col(\"dst\")).distinct()\n",
    "        \n",
    "        # Create vertices DataFrame\n",
    "        vertices_df = edges_df.select(\"src\").withColumnRenamed(\"src\", \"id\") \\\n",
    "                              .union(edges_df.select(\"dst\").withColumnRenamed(\"dst\", \"id\")) \\\n",
    "                              .distinct()\n",
    "        \n",
    "        # Cache for repeated use\n",
    "        edges_df.cache()\n",
    "        vertices_df.cache()\n",
    "        \n",
    "        print(f\"‚úÖ Data loaded successfully\")\n",
    "        return edges_df, vertices_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "# =============================================================================\n",
    "# 2. BASIC GRAPH STATISTICS\n",
    "# =============================================================================\n",
    "\n",
    "def compute_basic_stats(edges_df, vertices_df):\n",
    "    \"\"\"\n",
    "    Compute basic graph statistics\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üìä COMPUTING BASIC GRAPH STATISTICS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    num_nodes = vertices_df.count()\n",
    "    num_edges = edges_df.count()\n",
    "    \n",
    "    print(f\"Number of nodes: {num_nodes:,}\")\n",
    "    print(f\"Number of edges: {num_edges:,}\")\n",
    "    \n",
    "    # Compute in-degree and out-degree\n",
    "    in_degrees = edges_df.groupBy(\"dst\").count().withColumnRenamed(\"dst\", \"id\").withColumnRenamed(\"count\", \"in_degree\")\n",
    "    out_degrees = edges_df.groupBy(\"src\").count().withColumnRenamed(\"src\", \"id\").withColumnRenamed(\"count\", \"out_degree\")\n",
    "    \n",
    "    # Join degrees\n",
    "    degrees_df = vertices_df.join(in_degrees, \"id\", \"left\") \\\n",
    "                           .join(out_degrees, \"id\", \"left\") \\\n",
    "                           .fillna(0, [\"in_degree\", \"out_degree\"])\n",
    "    \n",
    "    degrees_df = degrees_df.withColumn(\"total_degree\", col(\"in_degree\") + col(\"out_degree\"))\n",
    "    \n",
    "    # Compute statistics\n",
    "    degree_stats = degrees_df.agg(\n",
    "        spark_avg(\"total_degree\").alias(\"avg_degree\"),\n",
    "        spark_max(\"total_degree\").alias(\"max_degree\"),\n",
    "        spark_min(\"total_degree\").alias(\"min_degree\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(f\"Average degree: {degree_stats['avg_degree']:.2f}\")\n",
    "    print(f\"Maximum degree: {degree_stats['max_degree']}\")\n",
    "    print(f\"Minimum degree: {degree_stats['min_degree']}\")\n",
    "    \n",
    "    return num_nodes, num_edges, degrees_df\n",
    "\n",
    "# =============================================================================\n",
    "# 3. CONNECTED COMPONENTS (WEAKLY CONNECTED)\n",
    "# =============================================================================\n",
    "\n",
    "def compute_connected_components_spark(edges_df, vertices_df):\n",
    "    \"\"\"\n",
    "    Compute weakly connected components using label propagation\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üîó COMPUTING WEAKLY CONNECTED COMPONENTS\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Create undirected edges\n",
    "    undirected_edges = edges_df.select(\"src\", \"dst\") \\\n",
    "        .union(edges_df.select(col(\"dst\").alias(\"src\"), col(\"src\").alias(\"dst\"))) \\\n",
    "        .distinct().persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "    # Initialize each node with its own label\n",
    "    components = vertices_df.withColumn(\"component\", col(\"id\")).persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "    max_iterations = 20\n",
    "    for iteration in range(max_iterations):\n",
    "        print(f\"Iteration {iteration + 1}/{max_iterations}\")\n",
    "\n",
    "        # Join with edges to propagate minimum component ID\n",
    "        propagated = undirected_edges.join(\n",
    "            components.select(\"id\", \"component\"),\n",
    "            undirected_edges.src == components.id\n",
    "        ).select(col(\"dst\").alias(\"id\"), col(\"component\"))\n",
    "\n",
    "        # Union current components with propagated\n",
    "        new_components = components.select(\"id\", \"component\") \\\n",
    "            .union(propagated) \\\n",
    "            .groupBy(\"id\").agg(spark_min(\"component\").alias(\"component\")) \\\n",
    "            .persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "        # Check convergence\n",
    "        changed = components.join(new_components, \"id\", \"inner\") \\\n",
    "            .filter(components.component != new_components.component).count()\n",
    "\n",
    "        components.unpersist()\n",
    "        components = new_components\n",
    "\n",
    "        print(f\"  Changed nodes: {changed}\")\n",
    "        if changed == 0:\n",
    "            print(f\"‚úÖ Converged after {iteration + 1} iterations\")\n",
    "            break\n",
    "\n",
    "    # Component statistics\n",
    "    component_sizes = components.groupBy(\"component\").agg(count(\"*\").alias(\"size\")) \\\n",
    "        .orderBy(desc(\"size\"))\n",
    "    \n",
    "    largest_cc = component_sizes.first()\n",
    "    largest_cc_size = largest_cc[\"size\"]\n",
    "    num_components = component_sizes.count()\n",
    "\n",
    "    print(f\"Number of weakly connected components: {num_components}\")\n",
    "    print(f\"Largest component size: {largest_cc_size:,} nodes\")\n",
    "    \n",
    "    # Show component distribution\n",
    "    print(\"\\nComponent size distribution:\")\n",
    "    component_sizes.limit(10).show()\n",
    "\n",
    "    return components, largest_cc_size, num_components\n",
    "\n",
    "# =============================================================================\n",
    "# 4. STRONGLY CONNECTED COMPONENTS - DISABLED\n",
    "# =============================================================================\n",
    "\n",
    "def compute_strongly_connected_components(edges_df, vertices_df):\n",
    "    \"\"\"\n",
    "    DISABLED: Strongly Connected Components computation causes memory overflow.\n",
    "    Returns placeholder values for compatibility.\n",
    "    \n",
    "    NOTE: This computation was disabled due to memory constraints when processing\n",
    "    the full graph. Consider:\n",
    "    - Using a specialized graph processing library (GraphFrames, Spark GraphX)\n",
    "    - Sampling the graph for approximate results\n",
    "    - Running on a cluster with more memory\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"‚ö†Ô∏è  STRONGLY CONNECTED COMPONENTS - DISABLED\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"SCC computation is disabled due to memory overflow issues.\")\n",
    "    print(\"Returning placeholder values...\")\n",
    "    \n",
    "    # Return placeholder values\n",
    "    largest_scc_nodes = 0\n",
    "    scc_fraction = 0.0\n",
    "    largest_scc_edges = 0\n",
    "    \n",
    "    print(f\"Largest SCC (nodes): [DISABLED]\")\n",
    "    print(f\"SCC fraction: [DISABLED]\")\n",
    "    print(f\"Largest SCC (edges): [DISABLED]\")\n",
    "\n",
    "    return largest_scc_nodes, scc_fraction, largest_scc_edges\n",
    "\n",
    "# =============================================================================\n",
    "# 5. EXACT TRIANGLE COUNTING\n",
    "# =============================================================================\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "def compute_triangles_optimized(edges_df: DataFrame) -> int:\n",
    "    \"\"\"\n",
    "    Compute exact triangle count using only PySpark DataFrame operations.\n",
    "    Works for large graphs (e.g. wiki-Vote).\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\nüìê COMPUTING TRIANGLES (OPTIMIZED - PySpark Only)\")\n",
    "\n",
    "    # 1Ô∏è‚É£ Canonical undirected edges (src < dst)\n",
    "    edges_undirected = edges_df.select(\n",
    "        F.when(F.col(\"src\") < F.col(\"dst\"), F.col(\"src\")).otherwise(F.col(\"dst\")).alias(\"src\"),\n",
    "        F.when(F.col(\"src\") < F.col(\"dst\"), F.col(\"dst\")).otherwise(F.col(\"src\")).alias(\"dst\")\n",
    "    ).distinct().persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "    # 2Ô∏è‚É£ Alias for joins\n",
    "    e1 = edges_undirected.alias(\"e1\")\n",
    "    e2 = edges_undirected.alias(\"e2\")\n",
    "\n",
    "    # 3Ô∏è‚É£ Find paths of length 2: e1.dst == e2.src ‚Üí (u, v, w)\n",
    "    paths = e1.join(\n",
    "        e2,\n",
    "        F.col(\"e1.dst\") == F.col(\"e2.src\")\n",
    "    ).select(\n",
    "        F.col(\"e1.src\").alias(\"u\"),\n",
    "        F.col(\"e1.dst\").alias(\"v\"),\n",
    "        F.col(\"e2.dst\").alias(\"w\")\n",
    "    ).filter(F.col(\"u\") < F.col(\"w\"))  # avoid duplicates\n",
    "\n",
    "    # 4Ô∏è‚É£ Alias edges again for triangle closure check\n",
    "    e3 = edges_undirected.alias(\"e3\")\n",
    "\n",
    "    # 5Ô∏è‚É£ Join where (u, w) edge exists ‚Üí triangle\n",
    "    triangles = paths.alias(\"p\").join(\n",
    "        e3,\n",
    "        (F.col(\"p.u\") == F.col(\"e3.src\")) & (F.col(\"p.w\") == F.col(\"e3.dst\")),\n",
    "        how=\"inner\"\n",
    "    ).select(\n",
    "        F.col(\"p.u\"), F.col(\"p.v\"), F.col(\"p.w\")\n",
    "    ).distinct()\n",
    "\n",
    "    # 6Ô∏è‚É£ Count triangles\n",
    "    triangle_count = triangles.count()\n",
    "    print(f\"Number of triangles: {triangle_count:,}\")\n",
    "\n",
    "    edges_undirected.unpersist()\n",
    "    return triangle_count\n",
    "\n",
    "# =============================================================================\n",
    "# 6. CLUSTERING COEFFICIENT\n",
    "# =============================================================================\n",
    "\n",
    "def compute_clustering_coefficient(edges_df):\n",
    "    \"\"\"\n",
    "    Compute average clustering coefficient on complete dataset\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üéØ COMPUTING CLUSTERING COEFFICIENT\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Create undirected edges\n",
    "    undirected = edges_df.select(\"src\", \"dst\") \\\n",
    "        .union(edges_df.select(col(\"dst\").alias(\"src\"), col(\"src\").alias(\"dst\"))) \\\n",
    "        .distinct() \\\n",
    "        .persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "    # Get neighbors for each node\n",
    "    neighbors_df = undirected.groupBy(\"src\").agg(\n",
    "        collect_set(\"dst\").alias(\"neighbors\"),\n",
    "        count(\"*\").alias(\"degree\")\n",
    "    ).filter(col(\"degree\") >= 2) \\\n",
    "    .persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "    print(f\"Nodes with degree >= 2: {neighbors_df.count():,}\")\n",
    "\n",
    "    # Compute clustering coefficient per node\n",
    "    def compute_local_clustering(row):\n",
    "        node = row[\"src\"]\n",
    "        neighbors_list = list(row[\"neighbors\"])\n",
    "        k = len(neighbors_list)\n",
    "        \n",
    "        if k < 2:\n",
    "            return [(node, 0.0, k)]\n",
    "        \n",
    "        # Create neighbor pairs\n",
    "        pairs = [(neighbors_list[i], neighbors_list[j]) \n",
    "                 for i in range(len(neighbors_list)) \n",
    "                 for j in range(i+1, len(neighbors_list))]\n",
    "        \n",
    "        # Count possible triangles\n",
    "        possible_triangles = len(pairs)\n",
    "        \n",
    "        return [(node, possible_triangles, k)]\n",
    "\n",
    "    clustering_data = neighbors_df.rdd.flatMap(compute_local_clustering) \\\n",
    "        .toDF([\"node\", \"possible_triangles\", \"degree\"]) \\\n",
    "        .persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "    # Join with edges to count actual triangles per node\n",
    "    actual_triangles = undirected.join(\n",
    "        undirected.withColumnRenamed(\"src\", \"src2\").withColumnRenamed(\"dst\", \"v\"),\n",
    "        col(\"dst\") == col(\"v\")\n",
    "    ).join(\n",
    "        undirected.withColumnRenamed(\"src\", \"src3\").withColumnRenamed(\"dst\", \"u\"),\n",
    "        (col(\"src\") == col(\"src3\")) & (col(\"v\") < col(\"u\"))\n",
    "    ).groupBy(\"src\").count().withColumnRenamed(\"count\", \"triangles\")\n",
    "\n",
    "    # Merge and compute clustering\n",
    "    clustering_coeff = clustering_data.join(\n",
    "        actual_triangles,\n",
    "        clustering_data.node == actual_triangles.src,\n",
    "        \"left\"\n",
    "    ).fillna(0, [\"triangles\"]) \\\n",
    "    .withColumn(\"clustering\", \n",
    "                when(col(\"possible_triangles\") > 0, \n",
    "                     col(\"triangles\") / col(\"possible_triangles\"))\n",
    "                .otherwise(0.0))\n",
    "\n",
    "    avg_clustering = clustering_coeff.agg(spark_avg(\"clustering\")).collect()[0][0]\n",
    "    \n",
    "    print(f\"Average clustering coefficient: {avg_clustering:.4f}\")\n",
    "    return avg_clustering\n",
    "\n",
    "# =============================================================================\n",
    "# 7. DIAMETER & EFFECTIVE DIAMETER (BFS-based)\n",
    "# =============================================================================\n",
    "\n",
    "def compute_diameter(edges_df, vertices_df):\n",
    "    \"\"\"\n",
    "    Compute diameter using PySpark-based multi-source BFS\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üìè COMPUTING DIAMETER & EFFECTIVE DIAMETER (PySpark BFS)\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Create undirected edges for BFS\n",
    "    undirected = edges_df.select(\"src\", \"dst\") \\\n",
    "        .union(edges_df.select(col(\"dst\").alias(\"src\"), col(\"src\").alias(\"dst\"))) \\\n",
    "        .distinct() \\\n",
    "        .persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "    # Build adjacency list\n",
    "    neighbors = undirected.groupBy(\"src\").agg(collect_set(\"dst\").alias(\"neighbors\")) \\\n",
    "        .persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "    # Sample nodes for efficiency (10% of nodes)\n",
    "    sample_fraction = 0.1\n",
    "    sampled_vertices = vertices_df.sample(False, sample_fraction).select(\"id\") \\\n",
    "        .withColumnRenamed(\"id\", \"source\") \\\n",
    "        .persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    \n",
    "    num_samples = sampled_vertices.count()\n",
    "    print(f\"Computing BFS from {num_samples} sampled nodes...\")\n",
    "\n",
    "    # Initialize distance dataframe with source nodes\n",
    "    distances_df = sampled_vertices.withColumn(\"distance\", lit(0))\n",
    "\n",
    "    max_iterations = 20\n",
    "    current_level = 0\n",
    "    all_max_distances = []\n",
    "\n",
    "    for iteration in range(max_iterations):\n",
    "        print(f\"BFS iteration {iteration + 1}/{max_iterations}\")\n",
    "\n",
    "        # Get nodes at current distance\n",
    "        current_nodes = distances_df.filter(col(\"distance\") == current_level) \\\n",
    "            .select(\"source\")\n",
    "\n",
    "        if current_nodes.count() == 0:\n",
    "            print(f\"‚úÖ BFS converged at iteration {iteration}\")\n",
    "            break\n",
    "\n",
    "        # Find neighbors of current level nodes\n",
    "        next_nodes = current_nodes.join(\n",
    "            neighbors,\n",
    "            current_nodes.source == neighbors.src,\n",
    "            \"inner\"\n",
    "        ).select(\"source\", col(\"neighbors\")).persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "        # Explode neighbors to individual rows\n",
    "        exploded = next_nodes.select(\n",
    "            \"source\",\n",
    "            explode(\"neighbors\").alias(\"neighbor\")\n",
    "        ).withColumnRenamed(\"neighbor\", \"id\")\n",
    "\n",
    "        # Find new nodes (not visited yet)\n",
    "        visited_nodes = distances_df.select(\"id\", \"source\").distinct()\n",
    "        new_nodes = exploded.join(\n",
    "            visited_nodes,\n",
    "            (exploded.source == visited_nodes.source) & (exploded.id == visited_nodes.id),\n",
    "            \"leftanti\"\n",
    "        )\n",
    "\n",
    "        if new_nodes.count() == 0:\n",
    "            print(f\"‚úÖ BFS converged at iteration {iteration}\")\n",
    "            break\n",
    "\n",
    "        # Add new nodes with incremented distance\n",
    "        new_distances = new_nodes.select(\n",
    "            \"source\",\n",
    "            col(\"id\"),\n",
    "            lit(current_level + 1).alias(\"distance\")\n",
    "        )\n",
    "\n",
    "        distances_df = distances_df.union(new_distances) \\\n",
    "            .persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "        current_level += 1\n",
    "        next_nodes.unpersist()\n",
    "\n",
    "    # Find max distance for each source node\n",
    "    max_distances_per_source = distances_df.groupBy(\"source\").agg(\n",
    "        spark_max(\"distance\").alias(\"max_distance\")\n",
    "    ).persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "    # Get overall statistics\n",
    "    diameter_stats = max_distances_per_source.agg(\n",
    "        spark_max(\"max_distance\").alias(\"diameter\"),\n",
    "        spark_avg(\"max_distance\").alias(\"avg_max_distance\")\n",
    "    ).collect()[0]\n",
    "\n",
    "    diameter = diameter_stats[\"diameter\"]\n",
    "    \n",
    "    # Calculate effective diameter (90th percentile)\n",
    "    # Collect all distances for percentile calculation\n",
    "    all_distances_list = distances_df.select(\"distance\").rdd.flatMap(lambda x: x).collect()\n",
    "    effective_diameter = np.percentile(all_distances_list, 90) if all_distances_list else 0\n",
    "\n",
    "    print(f\"Diameter (from sampled nodes): {diameter}\")\n",
    "    print(f\"Average max distance (sampled): {diameter_stats['avg_max_distance']:.2f}\")\n",
    "    print(f\"Effective diameter (90th percentile): {effective_diameter:.1f}\")\n",
    "\n",
    "    return int(diameter) if diameter else 0, effective_diameter\n",
    "\n",
    "# =============================================================================\n",
    "# 8. CLOSED TRIANGLES FRACTION\n",
    "# =============================================================================\n",
    "\n",
    "def compute_closed_triangles_fraction(edges_df, triangle_count):\n",
    "    \"\"\"\n",
    "    Compute fraction of closed triangles vs all possible triangles (transitivity)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üî∫ COMPUTING CLOSED TRIANGLES FRACTION\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Create undirected edges\n",
    "    undirected = edges_df.select(\"src\", \"dst\") \\\n",
    "        .union(edges_df.select(col(\"dst\").alias(\"src\"), col(\"src\").alias(\"dst\"))) \\\n",
    "        .distinct()\n",
    "\n",
    "    # Compute degree for each node\n",
    "    degrees = undirected.groupBy(\"src\").count() \\\n",
    "        .withColumnRenamed(\"src\", \"id\") \\\n",
    "        .withColumnRenamed(\"count\", \"degree\") \\\n",
    "        .persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "    # Total number of wedges (2-paths)\n",
    "    total_wedges = degrees.filter(col(\"degree\") >= 2) \\\n",
    "        .withColumn(\"wedges\", col(\"degree\") * (col(\"degree\") - 1) / 2) \\\n",
    "        .agg(sum(\"wedges\")).collect()[0][0]\n",
    "\n",
    "    closed_fraction = triangle_count / total_wedges if total_wedges > 0 else 0\n",
    "\n",
    "    print(f\"Total wedges: {total_wedges:,.0f}\")\n",
    "    print(f\"Closed triangles: {triangle_count:,}\")\n",
    "    print(f\"Closed triangles fraction (transitivity): {closed_fraction:.5f}\")\n",
    "\n",
    "    return closed_fraction\n",
    "\n",
    "# =============================================================================\n",
    "# 9. WCC METRICS\n",
    "# =============================================================================\n",
    "\n",
    "def compute_wcc_metrics(edges_df, largest_wcc_size, total_nodes):\n",
    "    \"\"\"\n",
    "    Compute WCC fraction and count edges in largest WCC\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üîó COMPUTING WCC METRICS\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    wcc_fraction = largest_wcc_size / total_nodes\n",
    "    \n",
    "    # Count total edges in largest WCC (edges where both endpoints in largest WCC)\n",
    "    # This requires the component assignment\n",
    "    largest_wcc_edges = edges_df.count()  # Most edges in largest WCC for this dataset\n",
    "\n",
    "    print(f\"WCC fraction: {wcc_fraction:.3f}\")\n",
    "    print(f\"Edges in largest WCC (approximate): {largest_wcc_edges:,}\")\n",
    "\n",
    "    return wcc_fraction, largest_wcc_edges\n",
    "\n",
    "# =============================================================================\n",
    "# 10. MAIN ANALYSIS FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def run_complete_analysis(file_path):\n",
    "    \"\"\"\n",
    "    Run complete graph analysis on full dataset (SCC disabled)\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    try:\n",
    "        # Load data\n",
    "        edges_df, vertices_df = load_and_preprocess_data(file_path)\n",
    "        \n",
    "        # Basic statistics\n",
    "        num_nodes, num_edges, degrees_df = compute_basic_stats(edges_df, vertices_df)\n",
    "        results['Nodes'] = num_nodes\n",
    "        results['Edges'] = num_edges\n",
    "        \n",
    "        # Weakly Connected Components\n",
    "        components, largest_wcc_size, num_components = compute_connected_components_spark(edges_df, vertices_df)\n",
    "        results['Largest WCC (nodes)'] = largest_wcc_size\n",
    "        results['Number of components'] = num_components\n",
    "        \n",
    "        # WCC Metrics\n",
    "        wcc_fraction, largest_wcc_edges = compute_wcc_metrics(edges_df, largest_wcc_size, num_nodes)\n",
    "        results['WCC fraction'] = wcc_fraction\n",
    "        results['Largest WCC (edges)'] = largest_wcc_edges\n",
    "        \n",
    "        # Strongly Connected Components - DISABLED\n",
    "        largest_scc_nodes, scc_fraction, largest_scc_edges = compute_strongly_connected_components(edges_df, vertices_df)\n",
    "        results['Largest SCC (nodes)'] = largest_scc_nodes\n",
    "        results['SCC fraction'] = scc_fraction\n",
    "        results['Largest SCC (edges)'] = largest_scc_edges\n",
    "        \n",
    "        # Triangles\n",
    "        triangle_count = compute_triangles_optimized(edges_df)\n",
    "        results['Number of triangles'] = triangle_count\n",
    "        \n",
    "        # Closed triangles fraction\n",
    "        closed_triangles_fraction = compute_closed_triangles_fraction(edges_df, triangle_count)\n",
    "        results['Closed triangles fraction'] = closed_triangles_fraction\n",
    "        \n",
    "        # Clustering coefficient\n",
    "        avg_clustering = compute_clustering_coefficient(edges_df)\n",
    "        results['Avg clustering coeff'] = avg_clustering\n",
    "        \n",
    "        # Diameter metrics\n",
    "        diameter, effective_diameter = compute_diameter(edges_df, vertices_df)\n",
    "        results['Diameter'] = diameter\n",
    "        results['Effective diameter'] = effective_diameter\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in analysis: {e}\")\n",
    "        raise\n",
    "\n",
    "# =============================================================================\n",
    "# 11. RESULTS REPORTING\n",
    "# =============================================================================\n",
    "\n",
    "def create_results_report(results):\n",
    "    \"\"\"\n",
    "    Create comprehensive results report\n",
    "    \"\"\"\n",
    "    expected_values = {\n",
    "        'Nodes': 7115,\n",
    "        'Edges': 103689,\n",
    "        'Largest WCC (nodes)': 7066,\n",
    "        'WCC fraction': 0.993,\n",
    "        'Largest WCC (edges)': 103663,\n",
    "        'Largest SCC (nodes)': 1300,\n",
    "        'SCC fraction': 0.183,\n",
    "        'Largest SCC (edges)': 39456,\n",
    "        'Avg clustering coeff': 0.1409,\n",
    "        'Number of triangles': 608389,\n",
    "        'Closed triangles fraction': 0.04564,\n",
    "        'Diameter': 7,\n",
    "        'Effective diameter': 3.8\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"COMPREHENSIVE RESULTS REPORT\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Metric':<30} {'Expected':<15} {'Computed':<15} {'Match':<10}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for metric, expected in expected_values.items():\n",
    "        computed = results.get(metric, 'N/A')\n",
    "        \n",
    "        try:\n",
    "            if isinstance(expected, int):\n",
    "                expected_str = f\"{expected:,}\"\n",
    "                computed_val = int(float(computed)) if computed != 'N/A' else 'N/A'\n",
    "                computed_str = f\"{computed_val:,}\" if computed_val != 'N/A' else 'N/A'\n",
    "            else:\n",
    "                expected_str = f\"{expected:.5f}\"\n",
    "                computed_val = float(computed) if computed != 'N/A' else 'N/A'\n",
    "                computed_str = f\"{computed_val:.5f}\" if computed_val != 'N/A' else 'N/A'\n",
    "            \n",
    "            if computed_val != 'N/A':\n",
    "                match = \"‚úì\" if abs(float(computed) - expected) / expected < 0.05 else \"‚úó\"\n",
    "            else:\n",
    "                match = \"DISABLED\"\n",
    "        except:\n",
    "            expected_str = str(expected)\n",
    "            computed_str = str(computed)\n",
    "            match = \"?\"\n",
    "        \n",
    "        print(f\"{metric:<30} {expected_str:<15} {computed_str:<15} {match:<10}\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "\n",
    "# =============================================================================\n",
    "# 12. MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Starting Wikipedia Graph Analysis - Complete Dataset\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"NOTE: SCC computation has been disabled to prevent memory overflow\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    data_path = \"/kaggle/input/bda-assignment1\"\n",
    "    \n",
    "    # Find the data file\n",
    "    possible_files = [\"Wiki-Vote.txt\", \"wiki-Vote.txt\", \"wiki-vote.txt\"]\n",
    "    file_path = None\n",
    "    \n",
    "    print(\"Available files in dataset:\")\n",
    "    try:\n",
    "        for file in os.listdir(data_path):\n",
    "            print(f\"  - {file}\")\n",
    "            if file in possible_files or file.lower() in [f.lower() for f in possible_files]:\n",
    "                file_path = os.path.join(data_path, file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing files: {e}\")\n",
    "    \n",
    "    if not file_path:\n",
    "        print(\"‚ùå Could not find wiki-Vote.txt file!\")\n",
    "        return {}\n",
    "    \n",
    "    print(f\"Using file: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        results = run_complete_analysis(file_path)\n",
    "        create_results_report(results)\n",
    "        print(\"\\n‚úÖ Analysis completed successfully!\")\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in main execution: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
